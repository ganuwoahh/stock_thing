{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":62334,"databundleVersionId":6834706,"sourceType":"competition"},{"sourceId":6821412,"sourceType":"datasetVersion","datasetId":3923217}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install seaborn\n!pip install ta\n!pip install statsmodels\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-01T16:55:10.468591Z","iopub.execute_input":"2023-11-01T16:55:10.469772Z","iopub.status.idle":"2023-11-01T16:55:53.551183Z","shell.execute_reply.started":"2023-11-01T16:55:10.469724Z","shell.execute_reply":"2023-11-01T16:55:53.549485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Lets import all necessary libraries for the analysis","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nfrom ta import add_all_ta_features\nfrom ta.utils import dropna\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom ta.volatility import BollingerBands \nimport ta\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:53.556271Z","iopub.execute_input":"2023-11-01T16:55:53.556714Z","iopub.status.idle":"2023-11-01T16:55:53.566396Z","shell.execute_reply.started":"2023-11-01T16:55:53.556679Z","shell.execute_reply":"2023-11-01T16:55:53.565321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load Dataset","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\n\n# reading the database\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\ndata_test = pd.read_csv('/kaggle/input/ue21cs342aa2/test.csv')\n \n# printing the top 10 rows\ndisplay(data.head(10))\ndisplay(data_test.head(10))\ndata.shape\ndata_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:53.568076Z","iopub.execute_input":"2023-11-01T16:55:53.568512Z","iopub.status.idle":"2023-11-01T16:55:53.648368Z","shell.execute_reply.started":"2023-11-01T16:55:53.568468Z","shell.execute_reply":"2023-11-01T16:55:53.647067Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### There are 300 rows in training dataset and 100 rows in test dataset.","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:53.651911Z","iopub.execute_input":"2023-11-01T16:55:53.65268Z","iopub.status.idle":"2023-11-01T16:55:53.670547Z","shell.execute_reply.started":"2023-11-01T16:55:53.652641Z","shell.execute_reply":"2023-11-01T16:55:53.669053Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n \n\nplt.figure(figsize=(15,5))\nplt.plot(data['Close'])\nplt.plot(data['Open'])\n\nplt.title('Stock Market Price Prediction', fontsize=15)\nplt.ylabel('Price')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:53.672893Z","iopub.execute_input":"2023-11-01T16:55:53.673385Z","iopub.status.idle":"2023-11-01T16:55:54.128492Z","shell.execute_reply.started":"2023-11-01T16:55:53.673343Z","shell.execute_reply":"2023-11-01T16:55:54.127631Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### lets check if there are any duplicates in the dataset.","metadata":{}},{"cell_type":"code","source":"data.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:54.129886Z","iopub.execute_input":"2023-11-01T16:55:54.130419Z","iopub.status.idle":"2023-11-01T16:55:54.138057Z","shell.execute_reply.started":"2023-11-01T16:55:54.130389Z","shell.execute_reply":"2023-11-01T16:55:54.136941Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Lets check for the Null values in the dataset.","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:54.139687Z","iopub.execute_input":"2023-11-01T16:55:54.140256Z","iopub.status.idle":"2023-11-01T16:55:54.151831Z","shell.execute_reply.started":"2023-11-01T16:55:54.140223Z","shell.execute_reply":"2023-11-01T16:55:54.150916Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.sort_values('Date', inplace=True)\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:54.153135Z","iopub.execute_input":"2023-11-01T16:55:54.153666Z","iopub.status.idle":"2023-11-01T16:55:54.172087Z","shell.execute_reply.started":"2023-11-01T16:55:54.153637Z","shell.execute_reply":"2023-11-01T16:55:54.170675Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\nresult = seasonal_decompose(data['Close'], model='multiplicative')\n#multiplicative decomposition is suitable if the magnitude of the seasonality changes with the trend, while additive works well if the magnitude remains constant.\n\ntrend = result.trend\nseasonal = result.seasonal\nresidual = result.resid\n\n\nplt.figure(figsize=(10, 6))\nplt.subplot(411)\nplt.plot(data['Close'], label='Original')\nplt.legend()\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend()\n\nplt.subplot(413)\nplt.plot(seasonal, label='Seasonality')\nplt.legend()\n\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:54.173994Z","iopub.execute_input":"2023-11-01T16:55:54.174331Z","iopub.status.idle":"2023-11-01T16:55:56.311484Z","shell.execute_reply.started":"2023-11-01T16:55:54.174303Z","shell.execute_reply":"2023-11-01T16:55:56.31014Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Trend: The long-term progression of the time series exhibits an overall upward trajectory. However, a substantial spike is noticeable in 2017, followed by a gradual decline from 2019 onwards.\n\n### Seasonality: Evident in the data is a distinct monthly pattern, highlighting a clear seasonal component.\n\n### Residual: It's important to note that the residual component is not insignificant. Further analysis is needed to delve into this aspect of the data.","metadata":{}},{"cell_type":"code","source":"features = ['Open', 'Close', 'Volume']\n\nplt.subplots(figsize=(20,10))\n\nfor i, col in enumerate(features):\n    plt.subplot(2,3,i+1)\n    sb.distplot(data[col])\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:56.316299Z","iopub.execute_input":"2023-11-01T16:55:56.316695Z","iopub.status.idle":"2023-11-01T16:55:57.515897Z","shell.execute_reply.started":"2023-11-01T16:55:56.316656Z","shell.execute_reply":"2023-11-01T16:55:57.514097Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### visually explore the distribution of specific features ('Open,' 'Close,' and 'Volume') within the dataset. By creating subplots with histograms for each feature, it enables us to gain insights into the data's central tendencies and the spread of values. This is essential for understanding the statistical characteristics of these features, identifying potential outliers, and making informed decisions regarding data preprocessing or modeling. In essence, it provides a preliminary overview of the data's key attributes, aiding in data analysis and decision-making.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load your dataset into a pandas DataFrame\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n\n# Extract the two variables you want to correlate\nX = data[\"Open\"]\nY = data[\"Close\"]\nZ = data[\"Volume\"]\n# Calculate the Pearson correlation coefficient\ncorrelation = X.corr(Y)\ncorrelation2 = Z.corr(Y)\ncorrelation3 = X.corr(Z)\n# Create a scatter plot\nplt.figure(figsize=(8, 6))\nplt.scatter(X, Y, alpha=0.7, color='b', label=f'Correlation: {correlation:.2f}')\nplt.scatter(Z, Y, alpha=0.7, color='b', label=f'Correlation2: {correlation:.2f}')\nplt.scatter(X, Z, alpha=0.7, color='b', label=f'Correlation3: {correlation:.2f}')\n# Add labels and a title\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Scatter Plot of X vs. Y\")\n\n# Add a legend\nplt.legend()\n\n# Add text annotation for the correlation coefficient\nplt.text(X.min(), Y.max(), f\"Pearson correlation: {correlation:.2f}\", fontsize=12, color='red')\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:57.517609Z","iopub.execute_input":"2023-11-01T16:55:57.518727Z","iopub.status.idle":"2023-11-01T16:55:58.089295Z","shell.execute_reply.started":"2023-11-01T16:55:57.518685Z","shell.execute_reply":"2023-11-01T16:55:58.088088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### we can see that Open and Close are linearly related.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load your dataset into a pandas DataFrame\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n\n# Extract the two variables you want to correlate\nX = data[\"Open\"]\nY = data[\"Close\"]\nZ = data[\"Volume\"]\n# Calculate the Pearson correlation coefficient\ncorrelation = X.corr(Z)\n\n# Create a scatter plot\nplt.figure(figsize=(8, 6))\nplt.scatter(X, Z, alpha=0.7, color='b', label=f'Correlation: {correlation:.2f}')\n\n# Add labels and a title\nplt.xlabel(\"X\")\nplt.ylabel(\"Z\")\nplt.title(\"Scatter Plot of X vs. Z\")\n\n# Add a legend\nplt.legend()\n\n# Add text annotation for the correlation coefficient\nplt.text(X.min(), Z.max(), f\"Pearson correlation: {correlation:.2f}\", fontsize=12, color='red')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:58.090603Z","iopub.execute_input":"2023-11-01T16:55:58.09151Z","iopub.status.idle":"2023-11-01T16:55:58.570558Z","shell.execute_reply.started":"2023-11-01T16:55:58.091475Z","shell.execute_reply":"2023-11-01T16:55:58.569158Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### we can see that Volume and Open are not related.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load your dataset into a pandas DataFrame\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n\n# Extract the two variables you want to correlate\nX = data[\"Open\"]\nY = data[\"Close\"]\nZ = data[\"Volume\"]\n# Calculate the Pearson correlation coefficient\ncorrelation = Y.corr(Z)\n\n# Create a scatter plot\nplt.figure(figsize=(8, 6))\nplt.scatter(Y, Z, alpha=0.7, color='b', label=f'Correlation: {correlation:.2f}')\n\n# Add labels and a title\nplt.xlabel(\"Y\")\nplt.ylabel(\"Z\")\nplt.title(\"Scatter Plot of X vs. Z\")\n\n# Add a legend\nplt.legend()\n\n# Add text annotation for the correlation coefficient\nplt.text(Y.min(), Z.max(), f\"Pearson correlation: {correlation:.2f}\", fontsize=12, color='red')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:58.572174Z","iopub.execute_input":"2023-11-01T16:55:58.573044Z","iopub.status.idle":"2023-11-01T16:55:59.045874Z","shell.execute_reply.started":"2023-11-01T16:55:58.573006Z","shell.execute_reply":"2023-11-01T16:55:59.044841Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we can see that Volume and Close are not related","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load your dataset into a pandas DataFrame\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n\nnumeric_features = data.drop(['id','Date','Strategy'], axis=1)\ncorrelation_matrix = numeric_features.corr()\n# Extract the two variables you want to correlate\n# X = data[\"Open\"]\n# Y = data[\"Close\"]\n# Z = data[\"Volume\"]\n\n\n# correlation_matrix = data.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title(\"Correlation Heatmap\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:59.047285Z","iopub.execute_input":"2023-11-01T16:55:59.04761Z","iopub.status.idle":"2023-11-01T16:55:59.456573Z","shell.execute_reply.started":"2023-11-01T16:55:59.047583Z","shell.execute_reply":"2023-11-01T16:55:59.455175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### we can see that open and close are highly correlated \n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n#Plotting ACF and PCF for the Close column before differncing\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# Assuming 'Close' is the name of your DataFrame column\nclose_series = df['Close']\n\n# Create ACF and PACF plots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# ACF plot\nsm.graphics.tsa.plot_acf(close_series, lags=40, ax=ax1)\nax1.set_title('Autocorrelation Function (ACF)')\n\n# PACF plot\nsm.graphics.tsa.plot_pacf(close_series, lags=40, ax=ax2)\nax2.set_title('Partial Autocorrelation Function (PACF)')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:55:59.458803Z","iopub.execute_input":"2023-11-01T16:55:59.459288Z","iopub.status.idle":"2023-11-01T16:56:00.541309Z","shell.execute_reply.started":"2023-11-01T16:55:59.459245Z","shell.execute_reply":"2023-11-01T16:56:00.53989Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### A gradually decreasing ACF plot indicates that there is a long-term dependency between data points, this often points to the presence of a trend. this proves that the data is non-stationary sharp cutoff suggests that there is a strong, direct relationship between the time series and its previous lags, but not with lags further back in time. so we do differencing of the series","metadata":{}},{"cell_type":"code","source":"#we perform ADF test to confirm the series is non-stationary\nimport statsmodels.api as sm\n\n# Assuming 'Close,' 'Open,' and 'Volume' are the column names in your DataFrame\nclose_series = df['Close']\nopen_series = df['Open']\nvolume_series = df['Volume']\n\n# Perform ADF test for 'Close'\nadf_result_close = sm.tsa.adfuller(close_series)\nprint(\"ADF Test for 'Close':\")\nprint(f'ADF Statistic: {adf_result_close[0]}')\nprint(f'p-value: {adf_result_close[1]}')\nprint(f'Critical Values: {adf_result_close[4]}')\n\n# Perform ADF test for 'Open'\nadf_result_open = sm.tsa.adfuller(open_series)\nprint(\"\\nADF Test for 'Open':\")\nprint(f'ADF Statistic: {adf_result_open[0]}')\nprint(f'p-value: {adf_result_open[1]}')\nprint(f'Critical Values: {adf_result_open[4]}')\n\n# Perform ADF test for 'Volume'\nadf_result_volume = sm.tsa.adfuller(volume_series)\nprint(\"\\nADF Test for 'Volume':\")\nprint(f'ADF Statistic: {adf_result_volume[0]}')\nprint(f'p-value: {adf_result_volume[1]}')\nprint(f'Critical Values: {adf_result_volume[4]}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:00.543176Z","iopub.execute_input":"2023-11-01T16:56:00.543684Z","iopub.status.idle":"2023-11-01T16:56:00.593928Z","shell.execute_reply.started":"2023-11-01T16:56:00.543612Z","shell.execute_reply":"2023-11-01T16:56:00.592592Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### we perform ADF test to confirm the series is non-stationary\n### From the output we cn say that Close an Open are  non-stationary because p value>0.05 this indicates that the  null hypothesis is accepted and the series is non-stationary  the p value for volume is <0.05 and it is stationary","metadata":{}},{"cell_type":"code","source":"#now we do differencing  for Close to convdert it to staionary\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Assuming you have a DataFrame 'df' with columns 'Date', 'Close', 'Open', and 'Volume'\n\n# Step 1: Differencing\ndf['Close_diff'] = df['Close'] - df['Close'].shift(1)\n\n# Step 2: ADF Test\nadf_result = adfuller(df['Close_diff'].dropna(), autolag='AIC')\nprint(\"ADF Test for 'Close_diff':\")\nprint(f\"ADF Statistic: {adf_result[0]}\")\nprint(f\"p-value: {adf_result[1]}\")\nprint(\"Critical Values:\", adf_result[4])\n\n# Step 3: ACF and PACF Plots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# ACF Plot\nplot_acf(df['Close_diff'].dropna(), lags=40, ax=ax1)\nax1.set_title('ACF Plot')\n\n# PACF Plot\nplot_pacf(df['Close_diff'].dropna(), lags=40, ax=ax2)\nax2.set_title('PACF Plot')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:00.59582Z","iopub.execute_input":"2023-11-01T16:56:00.596307Z","iopub.status.idle":"2023-11-01T16:56:01.460101Z","shell.execute_reply.started":"2023-11-01T16:56:00.596263Z","shell.execute_reply":"2023-11-01T16:56:01.458438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### After doing differencing  for CLose we get the p value as 0 ant the null hyphothesis is rejected the series is stationary and d value is 1 From the  ACF plot we get the q value  as 20 and p value 2  because of the sharp cut-off the graph ","metadata":{}},{"cell_type":"code","source":"##now we do differencing  for Open Column to convert it to staionary\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Assuming you have a DataFrame 'df' with columns 'Date', 'Close', 'Open', and 'Volume'\n\n# Step 1: Differencing\ndf['Open_diff'] = df['Open'] - df['Open'].shift(1)\n\n# Step 2: ADF Test\nadf_result = adfuller(df['Open_diff'].dropna(), autolag='AIC')\nprint(\"ADF Test for 'Open_diff':\")\nprint(f\"ADF Statistic: {adf_result[0]}\")\nprint(f\"p-value: {adf_result[1]}\")\nprint(\"Critical Values:\", adf_result[4])\n\n# Step 3: ACF and PACF Plots\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n\n# ACF Plot\nplot_acf(df['Open_diff'].dropna(), lags=40, ax=ax1)\nax1.set_title('ACF Plot')\n\n# PACF Plot\nplot_pacf(df['Open_diff'].dropna(), lags=40, ax=ax2)\nax2.set_title('PACF Plot')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:01.461845Z","iopub.execute_input":"2023-11-01T16:56:01.462322Z","iopub.status.idle":"2023-11-01T16:56:02.252982Z","shell.execute_reply.started":"2023-11-01T16:56:01.462274Z","shell.execute_reply":"2023-11-01T16:56:02.251717Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### After doing differencing  for open the series is stationary and d value is 1 From the  ACF plot we get the q  value  and p value  because of the sharp cut-off the graph","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:02.254602Z","iopub.execute_input":"2023-11-01T16:56:02.255673Z","iopub.status.idle":"2023-11-01T16:56:02.265831Z","shell.execute_reply.started":"2023-11-01T16:56:02.255609Z","shell.execute_reply":"2023-11-01T16:56:02.264323Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Convert the 'Date' column to a datetime object","metadata":{}},{"cell_type":"code","source":"\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:02.267352Z","iopub.execute_input":"2023-11-01T16:56:02.267832Z","iopub.status.idle":"2023-11-01T16:56:02.278646Z","shell.execute_reply.started":"2023-11-01T16:56:02.267783Z","shell.execute_reply":"2023-11-01T16:56:02.277004Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Calculate technical indicators from ta","metadata":{}},{"cell_type":"code","source":"data = add_all_ta_features(data, 'Open', 'Open', 'Open', 'Open', 'Volume', fillna=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:02.280588Z","iopub.execute_input":"2023-11-01T16:56:02.281103Z","iopub.status.idle":"2023-11-01T16:56:02.592739Z","shell.execute_reply.started":"2023-11-01T16:56:02.28106Z","shell.execute_reply":"2023-11-01T16:56:02.591511Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define the train-validation split point (e.g., 80% for training)","metadata":{}},{"cell_type":"code","source":"split_point = int(len(data) * 0.8)\ntrain, valid = data[:split_point], data[split_point:]","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:02.594356Z","iopub.execute_input":"2023-11-01T16:56:02.594741Z","iopub.status.idle":"2023-11-01T16:56:02.602592Z","shell.execute_reply.started":"2023-11-01T16:56:02.594709Z","shell.execute_reply":"2023-11-01T16:56:02.601159Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Add lagged values of 'Open'","metadata":{}},{"cell_type":"code","source":"num_lags = 4  # Number of lagged values\nfor lag in range(1, num_lags + 1):\n    train[f'Open_Lag_{lag}'] = train['Open'].shift(lag)\n    valid[f'Open_Lag_{lag}'] = valid['Open'].shift(lag)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:02.604193Z","iopub.execute_input":"2023-11-01T16:56:02.604572Z","iopub.status.idle":"2023-11-01T16:56:02.625227Z","shell.execute_reply.started":"2023-11-01T16:56:02.604541Z","shell.execute_reply":"2023-11-01T16:56:02.623055Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Calculate Open Price Momentum","metadata":{}},{"cell_type":"code","source":"n_days = 7  # Number of days for momentum calculation\ntrain['Open_Momentum'] = train['Open'] - train['Open'].shift(n_days)\nvalid['Open_Momentum'] = valid['Open'] - valid['Open'].shift(n_days)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:02.627052Z","iopub.execute_input":"2023-11-01T16:56:02.628185Z","iopub.status.idle":"2023-11-01T16:56:02.643061Z","shell.execute_reply.started":"2023-11-01T16:56:02.62815Z","shell.execute_reply":"2023-11-01T16:56:02.641797Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Define the list of exogenous variables (including available features)","metadata":{}},{"cell_type":"code","source":"exog_features = ['Open', 'volatility_bbm', 'volatility_bbw', 'volatility_bbp', 'momentum_rsi', 'trend_macd', 'volatility_atr', 'volume_obv', 'Open_Momentum']\nexog_features += [f'Open_Lag_{lag}' for lag in range(1, num_lags + 1)]\n\nexog_train = train[exog_features]\nexog_valid = valid[exog_features]\ny_train = train['Close']\ny_valid = valid['Close']\n\nmean_values = exog_train.mean()\nexog_train = exog_train.fillna(mean_values)\nexog_valid = exog_valid.fillna(mean_values)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:02.644537Z","iopub.execute_input":"2023-11-01T16:56:02.645349Z","iopub.status.idle":"2023-11-01T16:56:02.674737Z","shell.execute_reply.started":"2023-11-01T16:56:02.645306Z","shell.execute_reply":"2023-11-01T16:56:02.673416Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Correlation Analysis On Features","metadata":{}},{"cell_type":"code","source":"\n\n# Select only numeric columns in the training dataset\nnumeric_columns = train.select_dtypes(include=['number'])\n\n# Calculate correlations between numeric features and the 'Close' variable\ncorrelations = numeric_columns.corrwith(train['Close'])\n\n# Sort the correlations in descending order\ncorrelations = correlations.sort_values(ascending=False)\n\n# Create a visually appealing heatmap without numeric values\nplt.figure(figsize=(14, 10))  # Increase the figsize for a larger heatmap\nsns.set(font_scale=1.2)\nsns.set_style(\"whitegrid\")\n\n# Define a custom color palette\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Create the heatmap without annotations\nheatmap = sns.heatmap(\n    numeric_columns[correlations.index].corr(),\n    annot=False,  # Do not display numeric values\n    cmap=cmap,\n    cbar=True,\n    cbar_kws={'label': 'Correlation'},\n    linewidths=0.5,\n)\n\nplt.title(\"Correlation Heatmap\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:02.676426Z","iopub.execute_input":"2023-11-01T16:56:02.676898Z","iopub.status.idle":"2023-11-01T16:56:04.087667Z","shell.execute_reply.started":"2023-11-01T16:56:02.676855Z","shell.execute_reply":"2023-11-01T16:56:04.085799Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Strong Positive Correlations**\n\n'Open_Lag_3,' 'Open_Lag_4,' 'volatility_kch,' and 'volatility_kcl' have very strong positive correlations with 'Close,' indicating a significant positive relationship.","metadata":{}},{"cell_type":"markdown","source":"**Strong Negative Correlations**\n\n'volatility_ui' and 'volatility_dcw' exhibit strong negative correlations with 'Close,' suggesting a strong negative relationship.","metadata":{}},{"cell_type":"markdown","source":"**Moderate Correlations:**\n\nMany features have moderate positive or negative correlations, indicating some degree of relationship with the closing price.","metadata":{}},{"cell_type":"markdown","source":"**Weak Correlations:**\n\n\nSeveral features have weak or no correlations with 'Close,' suggesting limited influence on the closing price.","metadata":{}},{"cell_type":"markdown","source":"**Detailed Insights:**\n\n* 'Open_Lag_3' and 'Open_Lag_4' show very strong positive correlations, suggesting that past opening prices, specifically 3 and 4 days ago, significantly influence the current closing price. This implies a time-series effect.\n\n* 'volatility_ui' and 'volatility_dcw' have strong negative correlations, indicating that higher values of these volatility measures tend to push the closing price lower, reflecting increased market risk.\n\n* Trend indicators like 'trend_ema_fast,' 'trend_sma_fast,' 'trend_ichimoku_conv,' and 'trend_ichimoku_a' have strong positive correlations, suggesting that changes in trends strongly affect the closing price.\n\n* Some features have relatively weaker correlations, indicating less influence on the closing price.","metadata":{}},{"cell_type":"markdown","source":"# Close price prediction model (sarimax)","metadata":{}},{"cell_type":"code","source":"!pip install ta\nimport pandas as pd\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nfrom ta import add_all_ta_features\nimport numpy as np\n\n# Load your dataset\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n\n# Convert the 'Date' column to a datetime object\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True)\n\n# Calculate technical indicators\ndata = add_all_ta_features(data, 'Open', 'Open', 'Open', 'Open', 'Volume', fillna=True)\n\n# Specify the SARIMAX order\np, d, q = 2, 1, 2\nP, D, Q, S = 0, 0, 0, 0\n\n# Create 'Open_Lag_3' by lagging the 'Open' feature by 3 time steps\ndata['Open_Lag_3'] = data['Open'].shift(3)\n\n# Manually select the feature names you want to include\n# Calculate additional features\ndata['Previous_High'] = data['Open'].shift(1).rolling(window=10).max()\ndata['Previous_Low'] = data['Open'].shift(1).rolling(window=10).min()\ndata['Previous_Close'] = data['Open'].shift(1)\ndata['MA_10'] = data['Open'].rolling(window=10).mean()\n\n# Replace missing values with Backward Fill\ndata['Open_Lag_3'].fillna(method='bfill', inplace=True)\ndata['Previous_High'].fillna(method='bfill', inplace=True)\ndata['Previous_Low'].fillna(method='bfill', inplace=True)\ndata['Previous_Close'].fillna(method='bfill', inplace=True)\ndata['MA_10'].fillna(method='bfill', inplace=True)\n\n# Define the train-validation split point (e.g., 80% for training)\nsplit_point = int(len(data) * 0.7)\ntrain, valid = data[:split_point], data[split_point:]\n\n# Manually select the updated feature names\nselected_feature_names = ['Open', 'Open_Lag_3', 'volatility_kch', 'volatility_bbm', 'momentum_rsi', 'trend_macd', 'volatility_atr', 'Previous_High', 'Previous_Low', 'Previous_Close', 'MA_10']\n\n# Create X-train and Y_train\nX_train = train[selected_feature_names]\ny_train = train['Close']\n\n# Define the target variable\nX_test = valid[selected_feature_names]\ny_test = valid['Close']\n\n# Build and fit the SARIMAX model using the updated selected features\narimax_model_selected = SARIMAX(endog=y_train, exog=X_train, order=(p, d, q), seasonal_order=(P, D, Q, S))\narimax_model_selected = arimax_model_selected.fit(disp=False)\n\n# Forecast on the entire dataset using the updated selected features\nforecast_sarimax = arimax_model_selected.predict(start=X_test.index[0], end=X_test.index[-1], exog=X_test)\n\n# Calculate SMAPE\nsmape = 100 * np.mean(np.abs(forecast_sarimax - y_test) / (np.abs(forecast_sarimax) + np.abs(y_test)))\nprint(f'Symmetric Mean Absolute Percentage Error (SMAPE): {smape:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:04.089798Z","iopub.execute_input":"2023-11-01T16:56:04.090553Z","iopub.status.idle":"2023-11-01T16:56:19.934228Z","shell.execute_reply.started":"2023-11-01T16:56:04.090508Z","shell.execute_reply":"2023-11-01T16:56:19.932361Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# To create close price model (Multiple Linear Regression)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Load your dataset\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n\n# Convert the 'Date' column to a datetime object\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True)\n\n# Calculate technical indicators\ndata = add_all_ta_features(data, 'Open', 'Open', 'Open', 'Open', 'Volume', fillna=True)\n\n\ndata['Open_Lag_3'] = data['Open'].shift(3)\ndata['Previous_High'] = data['Open'].shift(1).rolling(window=10).max()\ndata['Previous_Low'] = data['Open'].shift(1).rolling(window=10).min()\ndata['Previous_Close'] = data['Open'].shift(1)\ndata['MA_10'] = data['Open'].rolling(window=10).mean()\n# data['Close_Diff'] = data['Close'].diff()\n\n# Replace missing values with Backward Fill\ndata['Open_Lag_3'].fillna(method='bfill', inplace=True)\ndata['Previous_High'].fillna(method='bfill', inplace=True)\ndata['Previous_Low'].fillna(method='bfill', inplace=True)\ndata['Previous_Close'].fillna(method='bfill', inplace=True)\ndata['MA_10'].fillna(method='bfill', inplace=True)\n# data['Close_Diff'].fillna(method='bfill', inplace=True)\n\nn = 5  # You can adjust the period as needed\ndata['Lowest_Low'] = data['Open'].rolling(window=n).min()\ndata['Highest_High'] = data['Open'].rolling(window=n).max()\ndata['%K'] = ((data['Open'] - data['Lowest_Low']) / (data['Highest_High'] - data['Lowest_Low'])) * 100\n# Calculate %D as a 3-day moving average of %K\ndata['%D'] = data['%K'].rolling(window=3).mean()\n# Replace missing values with Backward Fill\ndata['%K'].fillna(method='bfill', inplace=True)\ndata['%D'].fillna(method='bfill', inplace=True)\n\n\ny = data['Close']\n\n# Manually select the updated feature names\n# selected_feature_names = ['Open', 'Open_Lag_3', 'volatility_kch', 'volatility_bbm', 'momentum_rsi', 'trend_macd', 'volatility_atr', 'Previous_High', 'Previous_Low', 'Previous_Close', 'MA_10']\nselected_feature_names = ['Open_Lag_3', 'volatility_kch', 'volatility_bbm', 'Previous_High', 'Previous_Close']\n# selected_feature_names += ['%K', '%D']\n\n# Create the feature matrix\nX = data[selected_feature_names]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n\n# Create and fit a Linear Regression model\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = lr_model.predict(X_test)\n\n# Calculate Mean Squared Error (MSE) and R-squared (R2) for evaluation\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Squared Error (MSE): {mse:.2f}')\nprint(f'R-squared (R2): {r2:.2f}')\nsmape = 100 * np.mean(np.abs(y_pred - y_test) / (np.abs(y_pred) + np.abs(y_test)))\nprint(smape)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:19.947711Z","iopub.execute_input":"2023-11-01T16:56:19.948857Z","iopub.status.idle":"2023-11-01T16:56:20.317895Z","shell.execute_reply.started":"2023-11-01T16:56:19.948796Z","shell.execute_reply":"2023-11-01T16:56:20.316727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating heatmap","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n# Calculate the correlation matrix\ncorr_features = selected_feature_names + [\"Close\"]\ncorrelation_matrix = data[corr_features].corr()\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:20.319354Z","iopub.execute_input":"2023-11-01T16:56:20.319731Z","iopub.status.idle":"2023-11-01T16:56:21.345091Z","shell.execute_reply.started":"2023-11-01T16:56:20.319701Z","shell.execute_reply":"2023-11-01T16:56:21.343864Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Combined MLR and SARIMAX for close price","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nfrom ta import add_all_ta_features\nimport numpy as np\n\n# Load your dataset\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n\n# Convert the 'Date' column to a datetime object\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True)\n\n# Calculate technical indicators\ndata = add_all_ta_features(data, 'Open', 'Open', 'Open', 'Open', 'Volume', fillna=True)\n\n# Specify the SARIMAX order\np, d, q = 1, 1, 2\nP, D, Q, S = 0, 0, 0, 0\n\n# Create 'Open_Lag_3' by lagging the 'Open' feature by 3 time steps\ndata['Open_Lag_3'] = data['Open'].shift(3)\n\n# Manually select the feature names you want to include\n# Calculate additional features\ndata['Previous_High'] = data['Open'].shift(1).rolling(window=10).max()\ndata['Previous_Low'] = data['Open'].shift(1).rolling(window=10).min()\ndata['Previous_Close'] = data['Open'].shift(1)\ndata['MA_10'] = data['Open'].rolling(window=10).mean()\n\n# Replace missing values with Backward Fill\ndata['Open_Lag_3'].fillna(method='bfill', inplace=True)\ndata['Previous_High'].fillna(method='bfill', inplace=True)\ndata['Previous_Low'].fillna(method='bfill', inplace=True)\ndata['Previous_Close'].fillna(method='bfill', inplace=True)\ndata['MA_10'].fillna(method='bfill', inplace=True)\n\n# Define the train-validation split point (e.g., 80% for training)\nsplit_point = int(len(data) * 0.8)\ntrain, valid = data[:split_point], data[split_point:]\n\n# Manually select the updated feature names\nselected_feature_names = ['Open', 'Open_Lag_3', 'volatility_kch', 'volatility_bbm', 'momentum_rsi', 'trend_macd', 'volatility_atr', 'Previous_High', 'Previous_Low', 'Previous_Close', 'MA_10']\nselected_feature_mlr = ['Open_Lag_3', 'volatility_kch', 'volatility_bbm', 'Previous_High', 'Previous_Close']\n\n# Create X-train and Y_train\nX_train_sarimax = train[selected_feature_names]\nX_train_mlr = train[selected_feature_mlr]\ny_train = train['Close']\n\n# Define the target variable\nX_test_sarimax = valid[selected_feature_names]\nX_test_mlr = valid[selected_feature_mlr]\ny_test = valid['Close']\n\n# Build and fit the SARIMAX model using the updated selected features\narimax_model_selected = SARIMAX(endog=y_train, exog=X_train_sarimax, order=(p, d, q), seasonal_order=(P, D, Q, S))\narimax_model_selected = arimax_model_selected.fit(disp=False)\n\n# Forecast on the entire dataset using the updated selected features\nforecast_sarimax = arimax_model_selected.predict(start=X_test_sarimax.index[0], end=X_test_sarimax.index[-1], exog=X_test_sarimax)\n\n# Create and fit a Linear Regression model\nlr_model = LinearRegression()\nlr_model.fit(X_train_mlr, y_train)\n\n# Make predictions on the test data\nlr_predictions = lr_model.predict(X_test_mlr)\n\ncombined_predictions = (0.5*forecast_sarimax + 0.5*lr_predictions)\n\nsmape = 100 * np.mean(np.abs(combined_predictions - y_test) / (np.abs(combined_predictions) + np.abs(y_test)))\nprint(smape)\n\n# Visualize Actual vs. Predicted values for the validation set for the ensemble\nplt.figure(figsize=(12, 6))\nplt.plot(valid.index, valid['Close'], label='Actual', color='blue')\nplt.plot(valid.index, combined_predictions, label='Ensemble', color='purple')\nplt.title('Actual vs. Predicted Values (Validation) - Ensemble')\nplt.xlabel('Date')\nplt.ylabel('Close Price')\nplt.legend()\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:21.346545Z","iopub.execute_input":"2023-11-01T16:56:21.346972Z","iopub.status.idle":"2023-11-01T16:56:23.774119Z","shell.execute_reply.started":"2023-11-01T16:56:21.346942Z","shell.execute_reply":"2023-11-01T16:56:23.7727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### For closed_data.csv (Combined MLR and SARIMAX)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nfrom ta import add_all_ta_features\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n\n# Load your dataset\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\ndata2 = pd.read_csv('/kaggle/input/ue21cs342aa2/test.csv')\n\n# Convert the 'Date' column to a datetime object\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.set_index('Date', inplace=True)\ndata2['Date'] = pd.to_datetime(data2['Date'])\ndata2.set_index('Date', inplace=True)\n\n# Calculate technical indicators\ndata = add_all_ta_features(data, 'Open', 'Open', 'Open', 'Open', 'Volume', fillna=True)\ndata2 = add_all_ta_features(data2, 'Open', 'Open', 'Open', 'Open', 'Volume', fillna=True)\n\n# Specify the SARIMAX order\np, d, q = 1, 1, 2\nP, D, Q, S = 0, 0, 0, 0\n\n# Create features\ndata['Open_Lag_3'] = data['Open'].shift(3)\ndata['Previous_High'] = data['Open'].shift(1).rolling(window=10).max()\ndata['Previous_Low'] = data['Open'].shift(1).rolling(window=10).min()\ndata['Previous_Close'] = data['Open'].shift(1)\ndata['MA_10'] = data['Open'].rolling(window=10).mean()\n\ndata2['Open_Lag_3'] = data2['Open'].shift(3)\ndata2['Previous_High'] = data2['Open'].shift(1).rolling(window=10).max()\ndata2['Previous_Low'] = data2['Open'].shift(1).rolling(window=10).min()\ndata2['Previous_Close'] = data2['Open'].shift(1)\ndata2['MA_10'] = data2['Open'].rolling(window=10).mean()\n\n# Replace missing values with Backward Fill\ndata['Open_Lag_3'].fillna(method='bfill', inplace=True)\ndata['Previous_High'].fillna(method='bfill', inplace=True)\ndata['Previous_Low'].fillna(method='bfill', inplace=True)\ndata['Previous_Close'].fillna(method='bfill', inplace=True)\ndata['MA_10'].fillna(method='bfill', inplace=True)\n\ndata2['Open_Lag_3'].fillna(method='bfill', inplace=True)\ndata2['Previous_High'].fillna(method='bfill', inplace=True)\ndata2['Previous_Low'].fillna(method='bfill', inplace=True)\ndata2['Previous_Close'].fillna(method='bfill', inplace=True)\ndata2['MA_10'].fillna(method='bfill', inplace=True)\n\n# Define the train-validation split point (e.g., 80% for training)\n# split_point = int(len(data) * 0.8)\n# train, valid = data[:split_point], data[split_point:]\n\n# Manually select the updated feature names\nselected_feature_names = ['Open', 'Open_Lag_3', 'volatility_kch', 'volatility_bbm', 'momentum_rsi', 'trend_macd', 'volatility_atr', 'Previous_High', 'Previous_Low', 'Previous_Close', 'MA_10']\nselected_feature_mlr = ['Open_Lag_3', 'volatility_kch', 'volatility_bbm', 'Previous_High', 'Previous_Close']\n\n# Create X-train and Y_train\nX_train_sarimax = data[selected_feature_names]\nX_train_mlr = data[selected_feature_mlr]\ny_train = data['Close']\n\n# Define the target variable\nX_test_sarimax = data2[selected_feature_names]\nX_test_mlr = data2[selected_feature_mlr]\n\n# Build and fit the SARIMAX model using the updated selected features\narimax_model_selected = SARIMAX(endog=y_train, exog=X_train_sarimax, order=(p, d, q), seasonal_order=(P, D, Q, S))\narimax_model_selected = arimax_model_selected.fit(disp=False)\n\n# Forecast on the entire dataset using the updated selected features\nforecast_sarimax = arimax_model_selected.predict(start=X_test_sarimax.index[0], end=X_test_sarimax.index[-1], exog=X_test_sarimax)\n\n# Create and fit a Linear Regression model\nlr_model = LinearRegression()\nlr_model.fit(X_train_mlr, y_train)\n\n# Make predictions on the test data\nlr_predictions = lr_model.predict(X_test_mlr)\n\ncombined_predictions = (0.2*forecast_sarimax + 0.8*lr_predictions)\n\n# smape = 100 * np.mean(np.abs(combined_predictions - y_test) / (np.abs(combined_predictions) + np.abs(y_test)))\n# print(smape)\n\n# # Visualize Actual vs. Predicted values for the validation set for the ensemble\n# plt.figure(figsize=(12, 6))\n# plt.plot(valid.index, valid['Close'], label='Actual', color='blue')\n# plt.plot(valid.index, combined_predictions, label='Ensemble', color='purple')\n# plt.title('Actual vs. Predicted Values (Validation) - Ensemble')\n# plt.xlabel('Date')\n# plt.ylabel('Close Price')\n# plt.legend()\n# plt.grid(True)\n\ndata2 = data2.assign(Close=combined_predictions)\ndata2 = data2[[\"id\", \"Open\", \"Volume\", \"Close\"]]\ndata2.to_csv('close_data.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:23.776116Z","iopub.execute_input":"2023-11-01T16:56:23.776739Z","iopub.status.idle":"2023-11-01T16:56:27.014375Z","shell.execute_reply.started":"2023-11-01T16:56:23.7767Z","shell.execute_reply":"2023-11-01T16:56:27.013165Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Correlation Analysis on Features for Ensemble Model","metadata":{}},{"cell_type":"code","source":"\n# Load the dataset from 'train.csv'\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n\n# Encode the 'Strategy' column using label encoding\nlabel_encoder = LabelEncoder()\ndata['Encoded_Strategy'] = label_encoder.fit_transform(data['Strategy'])\n\n# Create lag features for 'Open,' 'Close,' and 'Volume'\nlag_periods = 5\n\nfor lag in range(1, lag_periods + 1):\n    data[f'Open_lag_{lag}'] = data['Open'].shift(lag)\n    data[f'Close_lag_{lag}'] = data['Close'].shift(lag)\n    data[f'Volume_lag_{lag}'] = data['Volume'].shift(lag)\n\n# Create rolling statistics features\nrolling_window = 4\ndata['Rolling_Mean_Volume'] = data['Volume'].rolling(rolling_window).mean()\ndata['Rolling_Std_Volume'] = data['Volume'].rolling(rolling_window).std()\ndata['Rolling_Mean_Open'] = data['Open'].rolling(rolling_window).mean()\ndata['Rolling_Std_Open'] = data['Open'].rolling(rolling_window).std()\ndata['Rolling_Mean_Close'] = data['Close'].rolling(rolling_window).mean()\ndata['Rolling_Std_Close'] = data['Close'].rolling(rolling_window).std()\n\n# Calculate the Exponential Moving Averages (EMA) for 'Open' and 'Close' columns\nema_period = 2\ndata['EMA_Open'] = ta.trend.ema_indicator(data['Open'], window=ema_period)\ndata['EMA_Close'] = ta.trend.ema_indicator(data['Close'], window=ema_period)\n\n# Calculate the MACD and MACD Histogram\ndata['MACD'] = ta.trend.MACD(data['Close']).macd()\ndata['MACD_Histogram'] = ta.trend.MACD(data['Close']).macd_diff()\n\n# Calculate the On-Balance Volume (OBV)\ndata['OBV'] = ta.volume.on_balance_volume(data['Close'], data['Volume'])\n\n# Calculate the Rate of Change (ROC)\nroc_period = 13\ndata['ROC'] = ((data['Close'] - data['Close'].shift(roc_period)) / data['Close'].shift(roc_period)) * 100\n\n# Calculate the Relative Strength Index (RSI) on 'Close'\nrsi_period = 14\ndata['RSI_Close'] = ta.momentum.RSIIndicator(data['Close'], window=rsi_period).rsi()\n\n# Calculate the Relative Strength Index (RSI) on 'Open'\ndata['RSI_Open'] = ta.momentum.RSIIndicator(data['Open'], window=rsi_period).rsi()\n\nbollinger_window = 20  # Adjust the window size as needed\nbollinger_bands = BollingerBands(close=data['Close'], window=bollinger_window)\ndata['volatility_bvp'] = bollinger_bands.bollinger_pband()\n\n# Create a list of feature columns including 'volatility_bvp'\nfeature_columns = ['Open', 'Close', 'Volume', 'EMA_Open', 'EMA_Close', 'MACD_Histogram', 'OBV', 'ROC', 'RSI_Close', 'RSI_Open', 'volatility_bvp'] + [f'Open_lag_{lag}' for lag in range(1, lag_periods + 1)] + [f'Close_lag_{lag}' for lag in range(1, lag_periods + 1)] + [f'Volume_lag_{lag}' for lag in range(1, lag_periods + 1)] + ['Rolling_Mean_Volume', 'Rolling_Std_Volume', 'Rolling_Mean_Open', 'Rolling_Std_Open', 'Rolling_Mean_Close', 'Rolling_Std_Close']\n\n# Calculate the correlations between features and the encoded 'Strategy'\ncorrelations = data[feature_columns + ['Encoded_Strategy']].corr()\n\n# Create a visually appealing heatmap with no numbers\nplt.figure(figsize=(12, 8))\nsns.set(font_scale=1.2)  # Adjust font size\nsns.heatmap(correlations, annot=False, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Heatmap')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:27.015877Z","iopub.execute_input":"2023-11-01T16:56:27.01624Z","iopub.status.idle":"2023-11-01T16:56:28.196145Z","shell.execute_reply.started":"2023-11-01T16:56:27.01621Z","shell.execute_reply":"2023-11-01T16:56:28.194973Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Insights and Inferences from Analysis","metadata":{}},{"cell_type":"markdown","source":"* 'Close' and 'Open' prices have moderate positive correlations with the encoded 'Strategy' (0.35 and 0.30, respectively), indicating their relevance in determining the strategy.\n\n* 'Volume' shows a negative correlation with the encoded 'Strategy' (-0.13), suggesting that high trading volumes may not always correlate positively with the strategy.\n\n* Exponential Moving Averages (EMAs) such as 'EMA_Open' and 'EMA_Close' exhibit moderate positive correlations (0.30 and 0.32, respectively), indicating their role in capturing short-term trends relevant to the strategy.\n\n* The 'MACD_Histogram' has a strong positive correlation (0.66) with the encoded 'Strategy,' indicating its significance in strategy determination. A high positive MACD Histogram may signify a bullish strategy.\n\n* The 'ROC' (Rate of Change) shows a very strong positive correlation (0.74) with the encoded 'Strategy,' suggesting that percentage price changes are crucial for strategy prediction.\n\n* Relative Strength Index (RSI), particularly 'RSI_Close' (0.82) and 'RSI_Open' (0.56), displays strong positive correlations with the strategy, implying that high RSI values are associated with specific strategies.\n\n* Lag features of 'Open' and 'Close' prices indicate that past price values are relevant, with positive correlations weakening as the lag period increases.\n\n* 'Volume_lag' features have negative correlations with the strategy, with decreasing strength as the lag period increases, implying the importance of recent trading volumes.\n\n* Rolling statistics, specifically 'Rolling_Mean_Open' (0.31) and 'Rolling_Mean_Close' (0.25), exhibit positive correlations, suggesting the relevance of rolling means of opening and closing prices. Conversely, 'Rolling_Std_Volume' has a negative correlation with the strategy.\n\n* 'volatility_bvp' stands out with the highest positive correlation (0.86) with the encoded 'Strategy,' indicating its critical role in determining specific strategies. High volatility appears to be a strong indicator.","metadata":{}},{"cell_type":"markdown","source":"### Model For Strategy","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport xgboost as xgb\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, classification_report\nimport ta\n\n# Load the dataset from 'train.csv'\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\n\n# Calculate RSI_Close feature\ndata['RSI_Close'] = ta.momentum.RSIIndicator(data['Close'], window=14).rsi()\n\n# Calculate ROC feature\ndata['ROC'] = ta.momentum.ROCIndicator(data['Close'], window=13).roc()\n\n# Calculate MACD and MACD_histogram features\ndata['MACD'] = ta.trend.MACD(data['Close']).macd()\ndata['MACD_histogram'] = ta.trend.MACD(data['Close']).macd_diff()\n\n# Calculate RSI_Open feature\ndata['RSI_Open'] = ta.momentum.RSIIndicator(data['Open'], window=14).rsi()\n\n# Calculate EMA_Close feature using pandas\ndata['EMA_Close'] = data['Close'].ewm(span=10, adjust=False).mean()\n\n# Calculate EMA_Open feature using pandas\ndata['EMA_open'] = data['Open'].ewm(span=10, adjust=False).mean()\n\n# Calculate Rolling Mean Open feature\ndata['Rolling Mean Open'] = data['Open'].rolling(window=10).mean()\n\n# Calculate Volatility_BVP feature\ndata['volatility_bvp'] = ta.volatility.BollingerBands(data['Close']).bollinger_pband()\n\n# Calculate 'Open - Close' difference feature\ndata['Open_Close_Difference'] = data['Open'] - data['Close']\n\n# Split the dataset into features (X) and the target variable (y) using the added features\nX = data[['RSI_Close', 'ROC', 'MACD', 'MACD_histogram', 'RSI_Open', 'EMA_Close', 'Rolling Mean Open', 'EMA_open', 'Close', 'volatility_bvp', 'Open_Close_Difference']]\ny = data['Strategy']\n\n# Handle missing values using SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nX = imputer.fit_transform(X)\n\n# Split the data into a training set and a testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n\n# Define the best hyperparameters for each classifier\nbest_rf_params = {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1}\nbest_xgb_params = {'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 3, 'n_estimators': 100, 'learning_rate': 0.1, 'subsample': 0.5}\n\n# Create individual classifiers with their best hyperparameters\nrf_classifier = RandomForestClassifier(random_state=42, **best_rf_params)\nxgb_classifier = xgb.XGBClassifier(random_state=42, **best_xgb_params)\n\n# Define the weights for each model\nweights = [0.50, 0.50]  # Adjust the weights for each classifier\n\n# Create an ensemble of classifiers using majority voting with weights\nensemble_classifier = VotingClassifier(estimators=[\n    ('rf', rf_classifier),\n    ('xgb', xgb_classifier)\n], voting='hard', weights=weights)\n\n# Fit the ensemble classifier on the training data\nensemble_classifier.fit(X_train, y_train)\n\n# Predict the target variable on the test set\ny_pred = ensemble_classifier.predict(X_test)\n\n# Calculate the accuracy as a percentage\naccuracy = accuracy_score(y_test, y_pred) * 100\n\n# Print the accuracy\nprint(f\"Accuracy with Ensemble Learning: {accuracy:.2f}%\")\n\n# Display the classification report\nreport = classification_report(y_test, y_pred)\nprint(\"\\nClassification Report:\\n\", report)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:28.197819Z","iopub.execute_input":"2023-11-01T16:56:28.198155Z","iopub.status.idle":"2023-11-01T16:56:28.637173Z","shell.execute_reply.started":"2023-11-01T16:56:28.198128Z","shell.execute_reply":"2023-11-01T16:56:28.636093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### For Strategy column insubmission.csv (actual)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nimport xgboost as xgb\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, classification_report\nimport ta\nimport lightgbm as lgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n\n# Load the dataset from 'train.csv'\ndata = pd.read_csv('/kaggle/input/ue21cs342aa2/train.csv')\ndata2 = pd.read_csv('/kaggle/working/close_data.csv')\n\n# Calculate some features\ndata['RSI_Close'] = ta.momentum.RSIIndicator(data['Close'], window=14).rsi()\ndata['ROC'] = ta.momentum.ROCIndicator(data['Close'], window=13).roc()\ndata['MACD'] = ta.trend.MACD(data['Close']).macd()\ndata['MACD_histogram'] = ta.trend.MACD(data['Close']).macd_diff()\ndata['RSI_Open'] = ta.momentum.RSIIndicator(data['Open'], window=14).rsi()\ndata['EMA_Close'] = data['Close'].ewm(span=10, adjust=False).mean()\ndata['EMA_open'] = data['Open'].ewm(span=10, adjust=False).mean()\ndata['Rolling Mean Open'] = data['Open'].rolling(window=10).mean()\ndata['volatility_bvp'] = ta.volatility.BollingerBands(data['Close']).bollinger_pband()\ndata['Open_Close_Difference'] = data['Open'] - data['Close']\n\ndata2['RSI_Close'] = ta.momentum.RSIIndicator(data2['Close'], window=14).rsi()\ndata2['ROC'] = ta.momentum.ROCIndicator(data2['Close'], window=13).roc()\ndata2['MACD'] = ta.trend.MACD(data2['Close']).macd()\ndata2['MACD_histogram'] = ta.trend.MACD(data2['Close']).macd_diff()\ndata2['RSI_Open'] = ta.momentum.RSIIndicator(data2['Open'], window=14).rsi()\ndata2['EMA_Close'] = data2['Close'].ewm(span=10, adjust=False).mean()\ndata2['EMA_open'] = data2['Open'].ewm(span=10, adjust=False).mean()\ndata2['Rolling Mean Open'] = data2['Open'].rolling(window=10).mean()\ndata2['volatility_bvp'] = ta.volatility.BollingerBands(data2['Close']).bollinger_pband()\ndata2['Open_Close_Difference'] = data2['Open'] - data2['Close']\n\n# Split the dataset into features (X) and the target variable (y) using the added features\nX_train = data[['RSI_Close', 'ROC', 'MACD', 'MACD_histogram', 'RSI_Open', 'EMA_Close', 'Rolling Mean Open', 'EMA_open', 'Close', 'volatility_bvp', 'Open_Close_Difference']]\nX_test = data2[['RSI_Close', 'ROC', 'MACD', 'MACD_histogram', 'RSI_Open', 'EMA_Close', 'Rolling Mean Open', 'EMA_open', 'Close', 'volatility_bvp', 'Open_Close_Difference']]\ny_train = data['Strategy']\n\n# Handle missing values using SimpleImputer\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\n\n# Define the best hyperparameters for each classifier\nbest_rf_params = {'n_estimators': 300, 'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 1}\n# best_xgb_params = {'objective': 'multi:softprob', 'num_class': 3, 'max_depth': 3, 'n_estimators': 100, 'learning_rate': 0.1, 'subsample': 0.5}\n# \n# lgbm_params = {'boosting_type': 'gbdt',\n#     'objective': 'binary',\n#     'num_leaves': 31,\n#     'learning_rate': 0.05,\n#     'feature_fraction': 0.9,\n#     'bagging_fraction': 0.8,\n#     'bagging_freq': 5,\n#     'verbose': 0}\n\n# Create individual classifiers with their best hyperparameters\nrf_classifier = RandomForestClassifier(random_state=42, **best_rf_params)\ngb_classifier =  GradientBoostingClassifier(n_estimators=300, learning_rate=1.0, max_depth=3,max_leaf_nodes=2,warm_start =True, random_state=0)\n# xgb_classifier = xgb.XGBClassifier(random_state=42, **best_xgb_params)\nclf = make_pipeline(StandardScaler(), SVC(gamma='auto',break_ties=True, shrinking=False, C=10))\n# lgbm = lgb.LGBMClassifier(**lgbm_params)\n\n# Define the weights for each model\nweights = [0.33, 0.33, 0.34]  # Adjust the weights for each classifier\n# Create an ensemble of classifiers using majority voting with weights\nensemble_classifier = VotingClassifier(estimators=[\n    ('rf', rf_classifier),\n    ('clf', clf),\n    ('gb', gb_classifier),\n], voting='hard', weights=weights)\n\n# Fit the ensemble classifier on the training data\nensemble_classifier.fit(X_train, y_train)\n\n# Predict the target variable on the test set\ny_pred = ensemble_classifier.predict(X_test)\n\ndata2 = data2.assign(Strategy=y_pred)\ndata2 = data2[[\"id\",\"Date\", \"Close\", \"Strategy\"]]\ndata2.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-11-01T16:56:28.640913Z","iopub.execute_input":"2023-11-01T16:56:28.641253Z","iopub.status.idle":"2023-11-01T16:56:31.007131Z","shell.execute_reply.started":"2023-11-01T16:56:28.641225Z","shell.execute_reply":"2023-11-01T16:56:31.005882Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}